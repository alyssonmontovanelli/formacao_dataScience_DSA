{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00c8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64356e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alyss\\anaconda3\\lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'C:\\\\Users\\\\alyss\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-8ec00140-58c0-4763-8555-47abd1ec4ba6.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31808\\2561908575.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Create a socket stream on target ip:port and count the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# words in input stream of \\n delimited text (e.g. generated by 'nc')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocketTextStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'C:\\\\Users\\\\alyss\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-8ec00140-58c0-4763-8555-47abd1ec4ba6.json'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import datetime\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "\n",
    "\n",
    "def getSparkSessionInstance(sparkConf: SparkConf) -> SparkSession:\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=sparkConf)\\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: sql_network_wordcount.py <hostname> <port> \", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "    host, port = sys.argv[1:]\n",
    "    sc = SparkContext(appName=\"PythonSqlNetworkWordCount\")\n",
    "    ssc = StreamingContext(sc, 1)\n",
    "\n",
    "    # Create a socket stream on target ip:port and count the\n",
    "    # words in input stream of \\n delimited text (e.g. generated by 'nc')\n",
    "    lines = ssc.socketTextStream(host, int(port))\n",
    "    words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "    # Convert RDDs of the words DStream to DataFrame and run SQL query\n",
    "    def process(time: datetime.datetime, rdd: RDD[str]) -> None:\n",
    "        print(\"========= %s =========\" % str(time))\n",
    "\n",
    "        try:\n",
    "            # Get the singleton instance of SparkSession\n",
    "            spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "            # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "            rowRdd = rdd.map(lambda w: Row(word=w))\n",
    "            wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "            # Creates a temporary view using the DataFrame.\n",
    "            wordsDataFrame.createOrReplaceTempView(\"words\")\n",
    "\n",
    "            # Do word count on table using SQL and print it\n",
    "            wordCountsDataFrame = \\\n",
    "                spark.sql(\"select word, count(*) as total from words group by word\")\n",
    "            wordCountsDataFrame.show()\n",
    "        except BaseException:\n",
    "            pass\n",
    "\n",
    "    words.foreachRDD(process)\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc483b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
