rnorm(100, sd = 9), var2 = 1:100 +
rnorm(100, sd = 16))
# VERSÃO FINAL COM OTIMIZAÇÃO
ggplot(tips, aes(x = total_bill, y = tip)) +
geom_point(aes(color = sex)) +
geom_smooth(method = 'lm')
data = data.frame(cond = rep(c("Obs 1", "Obs 2"),
each = 10), var = 1:100 +
rnorm(100, sd = 9), var2 = 1:100 +
rnorm(100, sd = 16))
data = data.frame(cond = rep(c("Obs 1", "Obs 2"),
each = 10), var = 1:100 +
rnorm(100, sd = 9), var2 = 1:100 +
rnorm(100, sd = 16))
data
# Dados
data = data.frame(cond = rep(c("Obs 1", "Obs 2"),
each = 10), var1 = 1:100 +
rnorm(100, sd = 9), var2 = 1:100 +
rnorm(100, sd = 16))
# Plotagem
ggplot(data, aes(x = var1, y = var2))
# Plotagem
ggplot(data, aes(x = var1, y = var2)) + geom_point(shape = 1) +
geom_smooth(method = , color = "red", se = FALSE)
# Dados
data = data.frame(grupo = c('A', 'B', 'C', 'D'),
valor = c(33,62,56,67),
num_obs = c(100, 500, 459, 340))
# Gerando massa de dados
data$right = cumsum(data$num_obs) + 30 * c(0:(nrow(data)-1))
data$
data$right
data$left = data$right - data$num_obs
data
View(dataset_bank)
# Carregando dados
dataset_bank <- read.table("C:\formacao_dataScience_DSA_DADOS\04_machineLearning\Cap02\bank\bank-full.csv",
View(dataset_bank)
# Carregando dados
dataset_bank <- read.table("C:/formacao_dataScience_DSA_DADOS/04_machineLearning/Cap02/bank/bank-full.csv",
header = TRUE, sep = ";")
View(dataset_bank)
round(dataset_bank)
# função table() para conhecer melhor os dados
table(dataset_bank$job)
# Visualizando os valores da tabela de forma gráfica
library(dplyr)
library(ggplot2)
dataset_bank %>%
group_by(job)%.%
summarise(n = n())%>%
ggplot(aes(x = job, y = n))+
geom_bar(start = "identity")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
# Visualizando os valores da tabela de forma gráfica
library(dplyr)
library(ggplot2)
dataset_bank %>%
group_by(job)%.%
summarise(n = n())%>%
ggplot(aes(x = job, y = n))+
geom_bar(start = "identity")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
dataset_bank %>%
group_by(job)%>%
summarise(n = n())%>%
ggplot(aes(x = job, y = n))+
geom_bar(start = "identity")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
dataset_bank %>%
group_by(job)%>%
summarise(n = n())%>%
ggplot(aes(x = job, y = n))+
geom_bar(start = "identity")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
dataset_bank %>%
group_by(job)%>%
summarise(n = n())%>%
ggplot(aes(x = job, y = n))+
geom_bar(stat = "identity")+
theme(axis.text.x = element_text(angle = 90, hjust = 1))
dataset_bank %>%
group_by(job)%>%
summarise(n = n())%>%
ggplot(aes(x = job, y = n))+
geom_bar(stat = "identity")+
theme(axis.text.x = element_text(angle = 60, hjust = 1))
dataset_bank %>%
group_by(job)%>%
summarise(n = n())%>%
ggplot(aes(x = job, y = n))+
geom_bar(stat = "identity")+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Adicionando nova coluna ao conjunto de dados com o mutate()
dataset_bank <- dataset_bank %>%
mutate(technology_use =
case_when(job == 'admin' ~ 'medio',
job == 'admin' ~ 'medio',
job == 'admin' ~ 'medio',))
# Adicionando nova coluna ao conjunto de dados com o mutate()
dataset_bank <- dataset_bank %>%
mutate(technology_use =
case_when(job == 'admin' ~ 'medio',
job == 'blue-collar' ~ 'baixo',
job == 'entrepreneur' ~ 'alto',
job == 'housemaid' ~ 'baixo',
job == 'management' ~ 'medio',
job == 'retired' ~ 'baixo',
job == 'self-employed' ~ 'baixo',
job == 'services' ~ 'medio',
job == 'student' ~ 'alto',
job == 'technician' ~ 'alto',
job == 'unemployed' ~ 'baixo',
job == 'unknown' ~ 'baixo'))
View(dataset_bank)
# PLotando nova coluna
dataset_bank %>%
group_by(technology_use)%>%
summarise(n = n())%>%
ggplot(aes(x = technology_use, y = n))+
geom_bar(stat = "Conhecimento em Tecnologia")+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# PLotando nova coluna
dataset_bank %>%
group_by(technology_use)%>%
summarise(n = n())%>%
ggplot(aes(x = technology_use, y = n))+
geom_bar(stat = "identity")+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
######################################################
dataset_bank <- dataset_bank %>%
mutate(defaulted = ifelse(default == 'yes', 1, 0))
View(data)
View(dataset_bank)
library(caret)
bank.dummies <- data.frame(predict(dmy, newdata = dataset_bank))
dmy <- dummyVars(" ~ .", data = dataset_bank)
bank.dummies <- data.frame(predict(dmy, newdata = dataset_bank))
View(bank.dummies)
dataset_bank %>%
group_by(job, maritial)%>%
summarise(n = n())
dataset_bank %>%
group_by(job, marital)%>%
summarise(n = n())
dataset_bank %.%
group_by(job, martial) %>%
summarise(n=n()) %>%
ggplot(aes(x = job, y = n, fill = marital))+
geom_bar(stat = 'identity', position = 'dodge')+
theme(axis.text.x = element_text(angle = 60, hjust = 1))
dataset_bank %>%
group_by(job, martial) %>%
summarise(n=n()) %>%
ggplot(aes(x = job, y = n, fill = marital))+
geom_bar(stat = 'identity', position = 'dodge')+
theme(axis.text.x = element_text(angle = 60, hjust = 1))
dataset_bank %>%
group_by(job, martial) %>%
summarise(n=n()) %>%
ggplot(aes(x = job, y = n, fill = marital))+
geom_bar(stat = 'identity', position = 'dodge')+
theme(axis.text.x = element_text(angle = 60, hjust = 1))
dataset_bank %>%
group_by(job, marital) %>%
summarise(n=n()) %>%
ggplot(aes(x = job, y = n, fill = marital))+
geom_bar(stat = 'identity', position = 'dodge')+
theme(axis.text.x = element_text(angle = 60, hjust = 1))
# Utilizando dummyVars nesse conjunto sumarizado
dmy <- dummyVars( ~ job:marital, data = dataset_bank)
bank.cross <- predict(dmy, newdata = dataset_bank)
View(bank.cross)
# Pacotes
install.packages("tm")
install.packages("SnowballC")
install.packages('wordcloud')
install.packages("gmodels")
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
# Carregando os dados
dados <- read.csv("sms_spam.csv", stringsAsFactors = FALSE)
# Carregando os dados
dados <- read.csv("C:/formacao_dataScience_DSA_DADOS/04_machineLearning/Cap07_NaiveBayes/R/sms_spam.csv", stringsAsFactors = FALSE)
# Examinando a estrutura dos dados
str(dados)
View(dados)
# Convertendo para fator
dados$type <- factor(dados$type)
str(dados)
View(dados)
str(dados$type)
View(dados$type)
table(dados$type)
table(dados$type) # mostra a ocorrência de cada valor
###### CONSTRUINDO UM CORPUS
# devemos sempre transformar o ocnjunto de dados e cj de documentos
# para trabalhar com análise de textos
dados_corpus <- VCorpus(VectorSource(dados$text))
dados_corpus
str(dados_corpus)
View(dados_corpus)
print(dados_corpus)
inspect(dados_corpus[1:2])
inspect(dados_corpus[1:3])
# Ajustando a estrutura
as.character(dados_corpus[[1]])
lapply(dados_corpus[1:2], as.character)
# Limpeza do Corpous com tm_map()
dados_corpus_clean <- tm_map(dados_corpus, content_transformer(tolower)) # colocar todas as palavras em minusculas
# Diferenças entre o Corpus inicial e o COrpus após a limpeza
as.character(dados_corpus[1])
as.character(dados_corpus_clean[1])
# Outras etapas de limpeza
dados_corpus_clean <- tm_map(dados_corpus_clean, removeNumbers) # REMOVE NUMEROS
dados_corpus_clean <- tm_map(dados_corpus_clean, removeWords, stompwords()) # REMOVE STOP WORDS
dados_corpus_clean <- tm_map(dados_corpus_clean, removeWords, stopwords()) # REMOVE STOP WORDS
dados_corpus_clean <- tm_map(dados_corpus_clean, removeWords, stopwords()) # REMOVE STOP WORDS
dados_corpus_clean <- tm_map(dados_corpus_clean, removePunctuation) # REMOVE PONTUAÇÃO
# CRIANDO UMA FUNÇÃO PARA SUBSTITUIR AO INVÉS DE REMOVER PONTUAÇÃO
removePunctuation("hello...world")
replacePunctuation <- function(x){gsub("[[:punct:]]+", " ", x)}
replacePunctuation("hello..word")
replacePunctuation("hello...word")
replacePunctuation <- function(x){gsub("[[:punct:]]+", " ", x)} # Troca pontuação por espaço
replacePunctuation("hello...word")
# Word Stemming - tirando o verbo das palavras
wordStem(c("learn", learned, learning, learns))
# Aplicando STEM
dados_corpus_clean <- tm_map(dados_corpus_clean, stemDocument)
# Eliminando espaço em branco
dados_corpus_clean <- tm_map(dados_corpus_clean, stripWhitespace)
# EXAMINANDO A VERSAO FINAL DO CORPUS
lapply(dados_corpus[1:3], as. character)
# EXAMINANDO A VERSAO FINAL DO CORPUS
lapply(dados_corpus[1:3], as.character)
lapply(dados_corpus_clean[1:3], as.character)
dados_dtm <- DocumentTermMatrix(dados_corpus_clean)
# Solução alternartiva 2 - cira uma matriz esparsa  document-term direto a partir do Corpus
dados_dtm2 <- DocumentTermMatrix(dados_corpus, control = list(tolower = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
# Solução alternativa 3 - usando stop words customizadas a partir da função
dados_dtm3 <- DocumentTermMatrix(dados_corpus, control = list(tolower = TRUE,
removeNumbers = TRUE,
stopwords = function(x) { removeWords(x, stopwords()) },
removePunctuation = TRUE,
stemming = TRUE))
# Comparando os resultados
dados_dtm
dados_dtm2
dados_dtm3
##eSEPARANDO CONJUNTO DE TREINO E TESTE
dados_dtm_train <- dados_dtm[1:4169, ]
dados_dtm_teste <- dados_dtm[4170:5559, ]
# Labels (Variável target)
dados_train_labels <- dados[1:4169, ]$type
dados_test_labels <- dados[4170:5559, ]$type
# Verificando se a proporção de Spam é similar
prop.table(table(dados_train_labels))
prop.table(table(dados_test_labels))
# Word CLOUD
wordcloud(dados_corpus_clean, min.freq = 50, random.order = FALSE)
# Frequencia dos dados
sms_dtm_freq_train <- removeSparseTerms(dados_dtm_train, 0.999)
sms_dtm_freq_train
# Word CLOUD
wordcloud(dados_corpus_clean, min.freq = 50, random.order = FALSE)
# Indicador de Features para palavras frequentes
findFreqTerms(dados_dtm_train, 5)
# save frequently-appearing terms to a character vector
sms_freq_words <- findFreqTerms(dados_dtm_train, 5)
str(sms_freq_words)
# Criando subsets apenas com palavras mais frequentes
sms_dtm_freq_train <- dados_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- dados_dtm_test[ , sms_freq_words]
# Converte para fator
convert_counts <- function(x) {
print(x)
x <- ifelse(x > 0, "Yes", "No")
}
# apply() converte counts para colunas de dados de treino e de teste
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
# Confusion Matrix
CrossTable(sms_test_pred,
dados_test_labels,
prop.chisq = FALSE,
prop.t = FALSE,
prop.r = FALSE,
dnn = c('Previsto', 'Observado'))
# Avaliando o modelo
sms_test_pred <- predict(nb_classifier, sms_test)
nb_classifier <- naiveBayes(sms_train, dados_train_labels)
# apply() converte counts para colunas de dados de treino e de teste
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
sms_dtm_freq_test <- dados_dtm_test[ , sms_freq_words]
# Frequencia dos dados
sms_dtm_freq_train <- removeSparseTerms(dados_dtm_train, 0.999)
sms_dtm_freq_train
# Indicador de Features para palavras frequentes
findFreqTerms(dados_dtm_train, 5)
# save frequently-appearing terms to a character vector
sms_freq_words <- findFreqTerms(dados_dtm_train, 5)
str(sms_freq_words)
# Criando subsets apenas com palavras mais frequentes
sms_dtm_freq_train <- dados_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- dados_dtm_test[ , sms_freq_words]
sms_dtm_freq_test <- dados_dtm_teste[ , sms_freq_words]
# Converte para fator
convert_counts <- function(x) {
print(x)
x <- ifelse(x > 0, "Yes", "No")
}
# apply() converte counts para colunas de dados de treino e de teste
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
nb_classifier <- naiveBayes(sms_train, dados_train_labels)
# Avaliando o modelo
sms_test_pred <- predict(nb_classifier, sms_test)
sms_test  <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
getwd()
# Criando um dataframe
clima <- expand.grid(Tempo = c("Ensolarado", "Nublado", "Chuvoso"),
Temperatura = c("Quente", "Ameno", "Frio"),
Humidade = c("Alta", "Normal"),
Vento = c("Fraco", "Forte"))
# Visualizando
View(clima)
# Variável target
response <- c(1, 19, 4, 31, 16, 2, 11, 23, 35, 6, 24, 15, 18, 36)
# Gerando um vetor do tipo fator
play <- as.factor(c("Não Jogar", "Não Jogar", "Não Jogar", "Jogar", "Jogar", "Jogar",
"Não Jogar", "Não Jogar", "Não Jogar", "Jogar", "Jogar", "Jogar",
"Não Jogar", "Não Jogar"))
# Data frame Final
tennis <- data.frame(clima[response,],play)
View(tennis)
#--------------------------
# Carregando o pacote
install.packages('rpart')
library(rpart)
tennis_tree <- rpart(play ~ .,
data = tennis,
method = "class",
parms = list(split = "information"), # Ganho de Informação
control = rpart.control(minsplit - 1)) # Partições d
# Visualizando o ganho de informção (Entropia) de cada atributo
tennis_tree
tennis_tree <- rpart(play ~ .,
data = tennis,
method = "class",
parms = list(split = "information"), # Ganho de Informação
control = rpart.control(minsplit = 1)) # Partições d
# Visualizando o ganho de informção (Entropia) de cada atributo
tennis_tree
# Visualizando o ganho de informção (Entropia) de cada atributo
tennis_tree
# Para melhor visualização
install.packages("rpart.plot")
library(rpart.plot)
# PLOTAGEM
?prp
prp(tennis_tree, type = 0, extra = 1, under = TRUE, compress = TRUE)
# Criando o modelo
?rpart
tennis_tree <- rpart(play ~ .,
data = tennis,
method = "class",
parms = list(split = "gini"), # Ganho de Informação
control = rpart.control(minsplit = 1)) # Partições d
# Visualizando o ganho de informção (Entropia) de cada atributo
tennis_tree
prp(tennis_tree, type = 0, extra = 1, under = TRUE, compress = TRUE)
tennis_tree <- rpart(play ~ .,
data = tennis,
method = "class",
parms = list(split = "gini"), # Ganho de Informação
control = rpart.control(minsplit = 2)) # Partições d
prp(tennis_tree, type = 0, extra = 1, under = TRUE, compress = TRUE)
prp(tennis_tree, type = 0, extra = 1, under = TRUE, compress = TRUE)
prp(tennis_tree, type = 0, extra = 1, under = TRUE, compress = TRUE)
tennis_tree <- rpart(play ~ .,
data = tennis,
method = "class",
parms = list(split = "information"), # Ganho de Informação
control = rpart.control(minsplit = 2)) # Partições d
prp(tennis_tree, type = 0, extra = 1, under = TRUE, compress = TRUE)
# ------------------------------------------------------
# Fazendo PREVISÕES COM NOVOS DADOS
clima2 <- expand.grid(Tempo = c("Ensolarado", "Nublado", "Chuvoso"),
Temperatura = c("Quente", "Ameno", "Frio"),
Humidade = c("Alta", "Normal"),
Vento = c("Fraco", "Forte"))
# Variável target
response2 <- c(2, 20, 3, 33, 17, 4,5)
# novos dados
novos_dados <- data.frame(clima[responsa,.])
# novos dados
novos_dados <- data.frame(clima[responsa,])
# novos dados
novos_dados <- data.frame(clima[response2,])
View(novos_dados)
# PREVISÕES
predict(tennis_tree, novos dados)
# PREVISÕES
predict(tennis_tree, novos_dados)
getwd()
getwd()
setwd('C:\formacao_dataScience_DSA\04_machineLearning\Cap08_DecisionTree_RandomForest_Ensemble_pt1')
setwd('C:/formacao_dataScience_DSA/04_machineLearning/Cap08_DecisionTree_RandomForest_Ensemble_pt1')
getwd()
# Gerando o dataset
data(Titanic, package = "datasets")
View(Titanic)
# Carregando o pacote
library(rpart)
# Criando o modelo
?rpart
# Criando o dataframe
dataset <- as.data.frame(Titanic)
titanic_case <- rpart(Survived ~ Class + Sex + Age,
data = dataset,
weights = Freq,
method = "class",
parms = list(split = "information"),
control = rpart.control(minsplit = 5))
titanic_case
# Plotando
?rpart.plot
library(rpart.plot)
prp(titanic_case, type = 0, extra = 1, under = TRUE, compress = TRUE)
prp(titanic_case, type = 2, extra = 1, under = TRUE, compress = TRUE)
prp(titanic_case, type = 1, extra = 1, under = TRUE, compress = TRUE)
prp(titanic_case, type = 0, extra = 1, under = TRUE, compress = TRUE)
prp(titanic_case, type = 0, extra = 2, under = TRUE, compress = TRUE)
prp(titanic_case, type = 0, extra = 0, under = TRUE, compress = TRUE)
prp(titanic_case, type = 0, extra = 1, under = TRUE, compress = TRUE)
prp(titanic_case)
, type = 0, extra = 1, under = TRUE, compress = TRUE
prp(titanic_case, type = 0, extra = 1, under = TRUE, compress = TRUE)
####### APLICANDO PRUNNING (PODA) NA ÁRVORE DE DECISÃO
pruned_titanic_tree <- prune(titanic_case, cp = 0.02)
pruned_titanic_tree
# Antes do Prunning
prp(titanic_case, type = 0, extra = 1, under = TRUE, compress = TRUE)
# Depois do Prunning
prp(pruned_titanic_tree, type = 0, extra = 1, under = TRUE, compress = TRUE)
setwd('C:/formacao_dataScience_DSA/04_machineLearning/Cap08_DecisionTree_RandomForest_Ensemble_pt1')
getwd()
# Calculando a ENTROPIA de duas classes
-0.60 * log2(0.60) - 0.40 * log2(0.40)
-0.50 * log2(0.50) - 0.50 * log2(0.50)
# Dados
credit <- read.csv("C:/formacao_dataScience_DSA_DADOS/04_machineLearning/Cap08_DecisionTree_RandomForest_Ensemble/credito.csv")
View(credit)
str(credit)
# Verificando 2 atributos do cliente
table(credit$checking_balance)
table(credit$savings_balance)
# Verificando as caracterisiticas do crédito
summary(credit$months_loan_duration)
summary(credit$amount)
# Variável target
table(credit$default)
# DIVIDINDO DADOS DE TREINO E TESTE - utilizando sample
set.seed(123)
train_sample <- sample(1000, 900)
# Split dos dataframes
credit_train <- credit[train_sample, ]
credit_test <- credit[-train,sample, ]
credit_test <- credit[-train_sample, ]
# Verificando proporção dos dados de treino e teste
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
install.packages("C50")
library(C50)
?C5.0
# Criando e visualizando o modelo
credit_model <- C5.0(credit_train[-17], credit_train$default)
credit_model
# Criando e visualizando o modelo
credit_model <- C5.0(credit_train[-17], credit_train$default)
# Criando e visualizando o modelo
credit_model <- C5.0(credit_train, credit_train$default)
# Criando e visualizando o modelo
credit_train$default <- as.factor(credit_train$default)
credit_model <- C5.0(credit_train[-17], credit_train$default)
credit_model
# Informações detalhadas sobre a árvore
summary(credit_model)
# Avaliando a performance do modelo
credit_predict <- predict(credit_model, credit_test)
# Confusion Matrix
library(gmodels)
?CrossTable
CrossTable(credit_test$default,
credit_predict,
prop.chisq = FALSE,
prop.c = FALSE,
prop.r = FALSE,
dnn = c("Observado", "Previsto"))
# Aumentando a precisão com 10 tentativas
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10) #treina 10x
credit_boost10
summary(credit_boost10)
