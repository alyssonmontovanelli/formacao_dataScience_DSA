{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"red\">REDES NEURAIS ARTIFICAIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faremos 3 etapas de maneira individual, são elas:\n",
    "\n",
    "- Desenvolvimento do FOWARD PROPAGATION\n",
    "\n",
    "- Desenvolvimento da função de erro (LOSS)\n",
    "\n",
    "- Desenvolvimento da BACKWARD PROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"green\">Parte 1 - Implementando Uma Rede Neural Artifical Somente com Fórmulas Matemáticas (Sem Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Parte 1A - FOWARD PROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"red\">Função Para Inicialização dos Pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para Inicialização randômmica dos parâmetros do modelo\n",
    "\n",
    "def inicializa_parametros(dims_camada_entrada):\n",
    "\n",
    " # Dicionário para os parâmetros\n",
    " parameters = {}\n",
    "\n",
    " # Comprimento das dimensões das camadas\n",
    " comp = len(dims_camada_entrada)\n",
    "\n",
    " # Loop pelo comprimento\n",
    " for i in range(1,comp):\n",
    "\n",
    "  # Inicialização da materiz de pesos\n",
    "  parameters[\"W\" + str(i)] = np.random.randn(dims_camada_entrada[i],\n",
    "                                             dims_camada_entrada[i - 1]) * 0.01\n",
    "  \n",
    "  # Inicialização do bias\n",
    "  parameters[\"b\" + str(i)] = np.zeros((dims_camada_entrada[i], 1))\n",
    "\n",
    " return parameters   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"green\">Desenvolvendo a Função Sigmóide </font>\n",
    "\n",
    "A principal razão pela qual usamos a função sigmóide é porque ela permite converter números para valores entre 0 e 1. \n",
    "\n",
    "Portanto, é especialmente usada para modelos em que temos que prever a probabilidade como uma saída. Como a probabilidade de qualquer coisa existir apenas entre o intervalo de 0 e 1, sigmoide é a escolha certa. Algumas caracterísiticas da função sigmóide:\n",
    "\n",
    "- A função é diferenciável. Isso significa que podemos encontrar a inclinação da curva sigmóide em dois pontos.\n",
    "- A função sigmóide logística pode fazer com que uma rede neural fique presa no momento do treinamento.\n",
    "- A função softmax é uma função de ativação logística mais generalizada, utilizada para a classificação em várias classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Afinal, O Que é Derivada?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Cálculo, a derivada em um ponto de uma função y = f(x) representa a taxa de variação instantânea de y em relação a x neste ponto. \n",
    "\n",
    "Um exemplo típico é a função velocidade que representa a taxa de variação (derivada) da função espaço. Do mesmo modo, a função aceleração é a derivada da função velocidade. Geometricamente, a derivada no ponto x = a de y = f(x) representa a inclinação da reta tangente ao gráfico desta função no ponto (a, f(a)).\n",
    "\n",
    "A função que a cada ponto x associa a derivada neste ponto de f(x) é chamada de função derivada de f(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em cada ponto, a derivada de f(x) é a tangente do ângulo que a reta tangente à curva faz em relação ao eixo das abscissas. A reta é sempre tangente à curva azul; a tangente do ângulo que ela faz com o eixo das abscissas é a derivada. Note-se que a derivada é positiva quando verde, negativa quando vermelha, e zero quando preta.\n",
    "\n",
    "A derivada de uma função y = f(x) num ponto x = x0, é igual ao valor da tangente trigonométrica do ângulo formado pela tangente geométrica à curva representativa de y=f(x), no ponto x = x0, ou seja, a derivada é o coeficiente angular da reta tangente ao gráfico da função no ponto x0.\n",
    "\n",
    "A função derivada é representada por f'(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função Sigmóide \n",
    "\n",
    "def sigmoide(Z):\n",
    " A = 1 / (1 + np.exp(-Z)) # exp é o calculo com número de euler\n",
    " return A, Z\n",
    "\n",
    "# Essa função vai entregar resultado entre 0 e 1, \n",
    "# se for mais prox de 0, a saida sigmoide será 0, \n",
    "# se for mais próximo de 1, a saíde sigmoide será 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"green\">Desenvolvendo a Função ReLu </font>\n",
    "\n",
    "A principal razão pela qual usamos a função sigmóide é porque ela permite converter números para valores entre 0 e 1. \n",
    "\n",
    "Portanto, é especialmente usada para modelos em que temos que prever a probabilidade como uma saída. Como a probabilidade de qualquer coisa existir apenas entre o intervalo de 0 e 1, sigmoide é a escolha certa. Algumas caracterísiticas da função sigmóide:\n",
    "\n",
    "- A função é diferenciável. Isso significa que podemos encontrar a inclinação da curva sigmóide em dois pontos.\n",
    "- A função sigmóide logística pode fazer com que uma rede neural fique presa no momento do treinamento.\n",
    "- A função softmax é uma função de ativação logística mais generalizada, utilizada para a classificação em várias classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função ReLu (Rectified Linear Unit) \n",
    "\n",
    "def relu(Z):\n",
    " A = abs(Z * (Z > 0)) # exp é o calculo com número de euler\n",
    " return A, Z\n",
    "\n",
    "# Essa função vai entregar resultado entre 0 e 1, \n",
    "# se for mais prox de 0, a saida sigmoide será 0, \n",
    "# se for mais próximo de 1, a saíde sigmoide será 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo a Ativação Linear da Rellu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operação de Ativação\n",
    "# A é a matriz com os dados de entrada\n",
    "# W é a matriz de pesos\n",
    "# b é o bias\n",
    "\n",
    "def linear_activation(A, W, b):\n",
    " Z = np.dot(W, A) + b\n",
    " cache = (A, W, b)\n",
    " return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1,2,3]\n",
    "W = 0.02\n",
    "b = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12.02, 12.04, 12.06]), ([1, 2, 3], 0.02, 12))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_activation(A, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = \"green\">Construindo a Fowrward Propagation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movimento para frente (fowrward)\n",
    "\n",
    "def forward(A_prev, W, b, activation):\n",
    "\n",
    " # Se a função de ativação for sigmoid, entramos neste bloco\n",
    " if activation == \"sigmoid\":\n",
    "  Z, linear_cache = linear_activation(A_prev, W, b)\n",
    "  A, activation_cache = sigmoide(Z)\n",
    "\n",
    " # Se não, se for Relu, entramos nesse bloco\n",
    " elif activation == \"relu\":\n",
    "  Z, linear_cache = linear_activation(A_prev, W, b)\n",
    "  A, activation_cache = relu(Z)\n",
    "\n",
    " cache = (linear_cache, activation_cache)\n",
    "\n",
    " return A, cache "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"green\">Combinando Ativação e Propagação</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagação para frente\n",
    "\n",
    "# X = addos de entrada\n",
    "def foward_propagation(X, parameters):\n",
    "\n",
    " # Lista de valores anteriores (cache)\n",
    " caches = []\n",
    "\n",
    " # Dados de entrada\n",
    " A = X\n",
    "\n",
    " # Comprimento dos parametros\n",
    " L = len(parameters) // 2 # divide // para que seja inteira\n",
    "\n",
    " # Loop nas camadas intermediarias - por isso se utiliza RELU\n",
    " for i in range(1, L):\n",
    "\n",
    "  # Guarda o valor prévio de A\n",
    "  A_prev = A\n",
    "\n",
    "  # Executa o Foward\n",
    "  A, cache = forward(A_prev, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)], activation= \"relu\")\n",
    "\n",
    "  # Grava o cache\n",
    "  caches.append(cache)\n",
    "\n",
    " # Saída nna ultima camada\n",
    " A_last, cache = forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(i)], activation=\"sigmoid\")\n",
    "\n",
    " # Grava o cache\n",
    "\n",
    " return(A_last, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"green\">Desenvolvendo a Função de Custo</font>\n",
    "\n",
    "Onde comparamos os resultados do da passada para frente com os dados históricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
