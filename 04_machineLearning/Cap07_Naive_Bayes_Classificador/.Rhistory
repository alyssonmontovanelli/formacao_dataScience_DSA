# Pacotes
install.packages("tm")
install.packages("SnowballC")
install.packages('wordcloud')
install.packages("gmodels")
library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(gmodels)
# Carregando os dados
dados <- read.csv("C:/formacao_dataScience_DSA_DADOS/04_machineLearning/Cap07_NaiveBayes/R/sms_spam.csv", stringsAsFactors = FALSE)
# Examinando a estrutura dos dados
str(dados)
View(dados)
# Convertendo para fator
dados$type <- factor(dados$type)
str(dados$type)
table(dados$type) # mostra a ocorrência de cada valor
###### CONSTRUINDO UM CORPUS
# devemos sempre transformar o ocnjunto de dados e cj de documentos
# para trabalhar com análise de textos
dados_corpus <- VCorpus(VectorSource(dados$text))
View(dados_corpus)
print(dados_corpus)
inspect(dados_corpus[1:3])
# Ajustando a estrutura
as.character(dados_corpus[[1]])
lapply(dados_corpus[1:2], as.character)
###### CONSTRUINDO UM CORPUS
# devemos sempre transformar o ocnjunto de dados e cj de documentos
# para trabalhar com análise de textos
dados_corpus <- VCorpus(VectorSource(dados$text))
View(dados_corpus)
(dados_corpus)
insp
print(dados_corpus)
inspect(dados_corpus[1:3])
# Ajustando a estrutura
as.character(dados_corpus[[1]])
lapply(dados_corpus[1:2], as.character)
# Limpeza do Corpous com tm_map()
dados_corpus_clean <- tm_map(dados_corpus, content_transformer(tolower)) # colocar todas as palavras em minusculas
# Diferenças entre o Corpus inicial e o COrpus após a limpeza
as.character(dados_corpus[1])
as.character(dados_corpus_clean[1])
# Diferenças entre o Corpus inicial e o COrpus após a limpeza
as.character(dados_corpus[1])
as.character(dados_corpus_clean[1])
# Diferenças entre o Corpus inicial e o COrpus após a limpeza
as.character(dados_corpus[1])
as.character(dados_corpus_clean[1])
# Outras etapas de limpeza
dados_corpus_clean <- tm_map(dados_corpus_clean, removeNumbers) # REMOVE NUMEROS
dados_corpus_clean <- tm_map(dados_corpus_clean, removeWords, stopwords()) # REMOVE STOP WORDS
dados_corpus_clean <- tm_map(dados_corpus_clean, removePunctuation) # REMOVE PONTUAÇÃO
# CRIANDO UMA FUNÇÃO PARA SUBSTITUIR AO INVÉS DE REMOVER PONTUAÇÃO
removePunctuation("hello...world")
replacePunctuation <- function(x){gsub("[[:punct:]]+", " ", x)} # Troca pontuação por espaço
replacePunctuation("hello...word")
# Word Stemming - tirando o verbo das palavras
wordStem(c("learn", learned, learning, learns))
# Word Stemming - tirando o verbo das palavras
wordStem(c("learn", learned, learning, learns))
# Word Stemming - tirando o verbo das palavras
wordStem(c("learn", "learned", "learning", "learns"))
# Aplicando STEM - automatizar a generalização das palavras
dados_corpus_clean <- tm_map(dados_corpus_clean, stemDocument)
# Eliminando espaço em branco
dados_corpus_clean <- tm_map(dados_corpus_clean, stripWhitespace)
# EXAMINANDO A VERSAO FINAL DO CORPUS
lapply(dados_corpus[1:3], as.character)
lapply(dados_corpus_clean[1:3], as.character)
############ ------------------------------------------------- ###########################
# Criando uma matriz esparsa document-term
?DocumentTermMatrix
dados_dtm <- DocumentTermMatrix(dados_corpus_clean)
# Solução alternartiva 2 - cira uma matriz esparsa  document-term direto a partir do Corpus
dados_dtm2 <- DocumentTermMatrix(dados_corpus, control = list(tolower = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE))
# Solução alternativa 3 - usando stop words customizadas a partir da função
dados_dtm3 <- DocumentTermMatrix(dados_corpus, control = list(tolower = TRUE,
removeNumbers = TRUE,
stopwords = function(x) { removeWords(x, stopwords()) },
removePunctuation = TRUE,
stemming = TRUE))
# Comparando os resultados
dados_dtm
dados_dtm2
dados_dtm3
1
dados_dtm_teste <- dados_dtm[4170:5559, ]
# Labels (Variável target)
dados_train_labels <- dados[1:4169, ]$type
dados_test_labels <- dados[4170:5559, ]$type
# Verificando se a proporção de Spam é similar
prop.table(table(dados_train_labels))
prop.table(table(dados_test_labels))
# Word CLOUD
wordcloud(dados_corpus_clean, min.freq = 50, random.order = FALSE)
# Frequencia dos dados
sms_dtm_freq_train <- removeSparseTerms(dados_dtm_train, 0.999)
sms_dtm_freq_train
# Word CLOUD
wordcloud(dados_corpus_clean, min.freq = 50, random.order = FALSE)
# Frequencia dos dados
sms_dtm_freq_train <- removeSparseTerms(dados_dtm_train, 0.999)
sms_dtm_freq_train
# Indicador de Features para palavras frequentes
findFreqTerms(dados_dtm_train, 5)
##eSEPARANDO CONJUNTO DE TREINO E TESTE
dados_dtm_train <- dados_dtm[1:4169, ]
dados_dtm_teste <- dados_dtm[4170:5559, ]
# Labels (Variável target)
dados_train_labels <- dados[1:4169, ]$type
dados_test_labels <- dados[4170:5559, ]$type
# Verificando se a proporção de Spam é similar
prop.table(table(dados_train_labels))
prop.table(table(dados_test_labels))
# Word CLOUD
wordcloud(dados_corpus_clean, min.freq = 50, random.order = FALSE)
# Frequencia dos dados
sms_dtm_freq_train <- removeSparseTerms(dados_dtm_train, 0.999)
sms_dtm_freq_train
# Indicador de Features para palavras frequentes
findFreqTerms(dados_dtm_train, 5)
# Indicador de Features para palavras frequentes
findFreqTerms(dados_dtm_train, 5)
# Indicador de Features para palavras frequentes
findFreqTerms(dados_dtm_train, 5)
# save frequently-appearing terms to a character vector
sms_freq_words <- findFreqTerms(dados_dtm_train, 5)
str(sms_freq_words)
# Criando subsets apenas com palavras mais frequentes
sms_dtm_freq_train <- dados_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- dados_dtm_teste[ , sms_freq_words]
# Converte para fator
convert_counts <- function(x) {
print(x)
x <- ifelse(x > 0, "Yes", "No")
}
# apply() converte counts para colunas de dados de treino e de teste
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
nb_classifier <- naiveBayes(sms_train, dados_train_labels)
# Converte para fator
convert_counts <- function(x) {
print(x)
x <- ifelse(x > 0, "Yes", "No")
}
# apply() converte counts para colunas de dados de treino e de teste
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
# Criando subsets apenas com palavras mais frequentes
sms_dtm_freq_train <- dados_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- dados_dtm_teste[ , sms_freq_words]
# apply() converte counts para colunas de dados de treino e de teste
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
# Confusion Matrix
CrossTable(sms_test_pred,
dados_test_labels,
prop.chisq = FALSE,
prop.t = FALSE,
prop.r = FALSE,
dnn = c('Previsto', 'Observado'))
# Confusion Matrix
CrossTable(sms_test_pred,
dados_test_labels,
prop.chisq = FALSE,
prop.t = FALSE,
prop.r = FALSE,
dnn = c('Previsto', 'Observado'))
# Avaliando o modelo
sms_test_pred <- predict(nb_classifier, sms_test)
nb_classifier <- naiveBayes(sms_train, dados_train_labels)
# Avaliando o modelo
sms_test_pred <- predict(nb_classifier, sms_test)
sms_test  <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
